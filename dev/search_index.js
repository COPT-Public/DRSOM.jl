var documenterSearchIndex = {"docs":
[{"location":"basics/#basics","page":"Basics","title":"Basics","text":"","category":"section"},{"location":"basics/","page":"Basics","title":"Basics","text":"In this part, we give basic usage of DRSOM,","category":"page"},{"location":"#DRSOM.jl","page":"Home","title":"DRSOM.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DRSOM.jl is a Julia implementation of the Dimension-Reduced Second-Order Method for unconstrained smooth optimization. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nDRSOM.jl is now a suite of second-order algorithms, including the variants of original DRSOM and the HSODM: a Homogeneous Second-order Descent Method.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The original 2-dimensional DRSOM works with the following iteration:","category":"page"},{"location":"","page":"Home","title":"Home","text":"        x_k+1     = x_k- alpha_k^1 g_k +alpha_k^2 d_k ","category":"page"},{"location":"","page":"Home","title":"Home","text":"where  m_k^alpha(alpha) is a 2-dimensional quadratic approximation to f(x) using gradient g_k and Hessian information H_k, namely,","category":"page"},{"location":"","page":"Home","title":"Home","text":"  beginaligned\n    min_alpha inmathbbR^2 m_k(alpha) = f(x_k) + (c_k)^T alpha+frac12 alpha^T Q_k alpha\n    mathrmst alpha_G_k= sqrtalpha^T G_k alpha le Delta quad textrmwithquad G_k=leftbeginarraycc\nleft(g_kright)^T g_k  -left(g_kright)^T d_k \n-left(g_kright)^T d_k  left(d_kright)^T d_k\nendarrayright   \n  endaligned ","category":"page"},{"location":"","page":"Home","title":"Home","text":"and","category":"page"},{"location":"","page":"Home","title":"Home","text":"\nQ_k =beginbmatrix\n\t( g_k)^T H_k g_k   -( d_k)^T H_k g_k \n\t-( d_k)^T H_k g_k  ( d_k)^T H_k d_k\n\tendbmatrix in mathcal S^2 \nc_k =beginbmatrix\n\t-left g_kright^2 \n\t( g_k)^T d_k\n\tendbmatrix in mathbbR^2","category":"page"},{"location":"","page":"Home","title":"Home","text":"The differentiation is done by ForwardDiff and ReverseDiff \nThe subproblem is very easy to solve.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nNotably, DRSOM does not have to compute n-by-n Hessian H_k directly (of course, it is perfect if you can provide!). Instead, it requires Hessian-vector products (HVPs) or interpolation to contruct the quadratic model. The latter approach is now preferred.","category":"page"},{"location":"#Install-DRSOM.jl","page":"Home","title":"Install DRSOM.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DRSOM is now available at JuliaRegistries, simply try","category":"page"},{"location":"","page":"Home","title":"Home","text":"(v1.8) pkg> add DRSOM","category":"page"},{"location":"","page":"Home","title":"Home","text":"try the dev branch (most up-to-date)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(v1.8) pkg> add DRSOM#dev","category":"page"},{"location":"#Try-your-own-ideas","page":"Home","title":"Try your own ideas","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"tip: Tip\nTo try your own ideas with DRSOM.jl,  use local path mode:(v1.8) pkg> add path-to-DRSOM.jlor the dev command:(v1.8) pkg> dev DRSOM","category":"page"},{"location":"#Algorithms","page":"Home","title":"Algorithms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you just want help on a specific algorithm, see the Algorithm Reference page.","category":"page"},{"location":"#Known-issues","page":"Home","title":"Known issues","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DRSOM.jl is still under active development. Please add issues on GitHub.","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DRSOM.jl is licensed under the MIT License. Check LICENSE for more details","category":"page"},{"location":"#Acknowledgment","page":"Home","title":"Acknowledgment","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Special thanks go to the COPT team and Tianyi Lin (Darren) for helpful suggestions.","category":"page"},{"location":"#Developer","page":"Home","title":"Developer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Chuwen Zhang <chuwzhang@gmail.com>\nYinyu Ye     <yyye@stanford.edu>","category":"page"},{"location":"#Reference","page":"Home","title":"Reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You are welcome to cite our paper on DRSOM :)","category":"page"},{"location":"","page":"Home","title":"Home","text":"@misc{zhang_drsom_2022,\n\ttitle = {{DRSOM}: {A} {Dimension} {Reduced} {Second}-{Order} {Method} and {Preliminary} {Analyses}},\n\turl = {http://arxiv.org/abs/2208.00208},\n\tpublisher = {arXiv},\n\tauthor = {Zhang, Chuwen and Ge, Dongdong and Jiang, Bo and Ye, Yinyu},\n\tmonth = jul,\n\tyear = {2022},\n\tnote = {arXiv:2208.00208 [cs, math]},\n\tkeywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"and HSODM,","category":"page"},{"location":"","page":"Home","title":"Home","text":"@misc{zhang_homogenous_2022,\n\ttitle = {A {Homogenous} {Second}-{Order} {Descent} {Method} for {Nonconvex} {Optimization}},\n\turl = {http://arxiv.org/abs/2211.08212},\n\tpublisher = {arXiv},\n\tauthor = {Zhang, Chuwen and Ge, Dongdong and He, Chang and Jiang, Bo and Jiang, Yuntian and Xue, Chenyu and Ye, Yinyu},\n\tmonth = nov,\n\tyear = {2022},\n\tnote = {arXiv:2211.08212 [math]},\n\tkeywords = {Mathematics - Optimization and Control}\n}","category":"page"},{"location":"alg/#alg_reference_list","page":"Algorithm Reference","title":"Algorithm Reference","text":"","category":"section"},{"location":"alg/#Original-DRSOM-with-2-directions-g_k-and-d_k","page":"Algorithm Reference","title":"Original DRSOM with 2 directions g_k and d_k","text":"","category":"section"},{"location":"alg/","page":"Algorithm Reference","title":"Algorithm Reference","text":"note: Note\nWe do not currently support nonsmooth/composite optimization, this is a future plan.","category":"page"},{"location":"alg/","page":"Algorithm Reference","title":"Algorithm Reference","text":"DRSOM.DRSOMState","category":"page"},{"location":"alg/#DRSOM.DRSOMState","page":"Algorithm Reference","title":"DRSOM.DRSOMState","text":"Struct for DRSOM to keep the iterate state,     including the following attributes e.g.,\n\nx the current iterate, namely x;\nfx the function value, namely f(x);\n∇f the gradient, namely nabla f(x);\nt the running time;\nϵ gradient norm nabla f(x)\n\n\n\n\n\n","category":"type"},{"location":"alg/","page":"Algorithm Reference","title":"Algorithm Reference","text":"DRSOM.DRSOMIteration","category":"page"},{"location":"alg/#DRSOM.DRSOMIteration","page":"Algorithm Reference","title":"DRSOM.DRSOMIteration","text":"Iteration object for DRSOM, to initialize an iterator,      you must specify the attributes, including \n\nx_0 is initial point of the iterate                  \nf is the smooth function to minimize \nϕ is the nonsmooth function          \ng is the gradient function           \nga is gradient function via forward or backward diff \nhvp is hvp function via forward or backward diff\nH is hessian function \n\nrest of the attributes have default options:\n\n    t::Dates.DateTime = Dates.now()\n    itermax::Int64 = 20\n    fog = :forward\n    sog = :forward\n\n\n\n\n\n","category":"type"},{"location":"alg/","page":"Algorithm Reference","title":"Algorithm Reference","text":"note: Note\nThis TRS solver only works for low-dimensional subproblems","category":"page"},{"location":"alg/","page":"Algorithm Reference","title":"Algorithm Reference","text":"DRSOM.SimpleTrustRegionSubproblem","category":"page"},{"location":"alg/#DRSOM.SimpleTrustRegionSubproblem","page":"Algorithm Reference","title":"DRSOM.SimpleTrustRegionSubproblem","text":"A simple procedure to solve TRS via a given regularizer  This is only used in DRSOM\n\nthe radius free mode: compute the Lagrangian multiplier according to the adaptive gamma instead.    See the paper for details.\nthe strict regularization mode: strictly solve a quadratic regularization problems given a regularizer lambda \nthe strict radius mode: if you choose a radius, this function is a bisection procedure    following the complexity of O(loglog(1ε)) by Ye\n\nWe use this function (radius-free mode) only in DRSOM, since the eigenvalues are easy to solve. \n\n\n\n\n\n","category":"function"}]
}
